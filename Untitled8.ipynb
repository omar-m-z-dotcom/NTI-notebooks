{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN59dmergIo5a48oBzzp9Av"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"VFmZX6msrkOD","colab":{"base_uri":"https://localhost:8080/","height":513},"executionInfo":{"status":"error","timestamp":1713167738017,"user_tz":-120,"elapsed":1405,"user":{"displayName":"omar mohamed","userId":"14177253784881428067"}},"outputId":"4098815e-f694-429b-d456-acdd4f20398f"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Exception encountered when calling layer 'model_4' (type Functional).\n\nInput 0 of layer \"block1_conv1\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (507, 640, 3)\n\nCall arguments received by layer 'model_4' (type Functional):\n  • inputs=tf.Tensor(shape=(507, 640, 3), dtype=float32)\n  • training=None\n  • mask=None","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-d2295dd65f9e>\u001b[0m in \u001b[0;36m<cell line: 138>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;31m# Perform style transfer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0mstylized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstyle_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;31m# Plot the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-d2295dd65f9e>\u001b[0m in \u001b[0;36mstyle_transfer\u001b[0;34m(content_image, style_image, num_iterations, content_weight, style_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStyleContentModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mstyle_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mcontent_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-d2295dd65f9e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mpreprocessed_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         style_outputs, content_outputs = (outputs[:self.num_style_layers],\n\u001b[1;32m     54\u001b[0m                                           outputs[self.num_style_layers:])\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'model_4' (type Functional).\n\nInput 0 of layer \"block1_conv1\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (507, 640, 3)\n\nCall arguments received by layer 'model_4' (type Functional):\n  • inputs=tf.Tensor(shape=(507, 640, 3), dtype=float32)\n  • training=None\n  • mask=None"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","# Load pre-trained VGG19 model\n","vgg19 = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n","\n","# Content layer where will pull our feature maps\n","content_layers = ['block5_conv2']\n","\n","# Style layer we are interested in\n","style_layers = ['block1_conv1',\n","                'block2_conv1',\n","                'block3_conv1',\n","                'block4_conv1',\n","                'block5_conv1']\n","\n","num_content_layers = len(content_layers)\n","num_style_layers = len(style_layers)\n","\n","def vgg_layers(layer_names):\n","    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n","    # Load our model. Load pretrained VGG, trained on imagenet data\n","    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n","    vgg.trainable = False\n","\n","    outputs = [vgg.get_layer(name).output for name in layer_names]\n","\n","    model = tf.keras.Model([vgg.input], outputs)\n","    return model\n","\n","def gram_matrix(input_tensor):\n","    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n","    input_shape = tf.shape(input_tensor)\n","    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n","    return result/(num_locations)\n","\n","class StyleContentModel(tf.keras.models.Model):\n","    def __init__(self, style_layers, content_layers):\n","        super(StyleContentModel, self).__init__()\n","        self.vgg =  vgg_layers(style_layers + content_layers)\n","        self.style_layers = style_layers\n","        self.content_layers = content_layers\n","        self.num_style_layers = len(style_layers)\n","        self.vgg.trainable = False\n","\n","    def call(self, inputs):\n","        \"Expects float input in [0,1]\"\n","        inputs = inputs*255.0\n","        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n","        outputs = self.vgg(preprocessed_input)\n","        style_outputs, content_outputs = (outputs[:self.num_style_layers],\n","                                          outputs[self.num_style_layers:])\n","\n","        style_outputs = [gram_matrix(style_output)\n","                         for style_output in style_outputs]\n","\n","        content_dict = {content_name:value\n","                        for content_name, value\n","                        in zip(self.content_layers, content_outputs)}\n","\n","        style_dict = {style_name:value\n","                      for style_name, value\n","                      in zip(self.style_layers, style_outputs)}\n","\n","        return {'content':content_dict, 'style':style_dict}\n","\n","def clip_0_1(image):\n","    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n","\n","def style_transfer(content_image, style_image, num_iterations=1000, content_weight=1e3, style_weight=1e-2):\n","    extractor = StyleContentModel(style_layers, content_layers)\n","\n","    style_targets = extractor(style_image)['style']\n","    content_targets = extractor(content_image)['content']\n","\n","    # Add batch dimension to the images\n","    content_image = tf.expand_dims(content_image, axis=0)\n","    style_image = tf.expand_dims(style_image, axis=0)\n","\n","    image = tf.Variable(content_image)  # Initializing with content image\n","\n","    opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n","\n","    style_weight = style_weight\n","    content_weight = content_weight\n","\n","    for i in range(num_iterations):\n","        with tf.GradientTape() as tape:\n","            outputs = extractor(image)\n","            content_outputs = outputs['content']\n","            style_outputs = outputs['style']\n","\n","            style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2)\n","                                   for name in style_outputs.keys()])\n","            style_loss *= style_weight / num_style_layers\n","\n","            content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)\n","                                    for name in content_outputs.keys()])\n","            content_loss *= content_weight / num_content_layers\n","\n","            total_loss = style_loss + content_loss\n","\n","        grad = tape.gradient(total_loss, image)\n","        opt.apply_gradients([(grad, image)])\n","        image.assign(clip_0_1(image))\n","\n","        if i % 100 == 0:\n","            print(\"Iteration:\", i)\n","            print(\"Total loss:\", total_loss)\n","            print(\"Style loss:\", style_loss)\n","            print(\"Content loss:\", content_loss)\n","\n","    # Remove the batch dimension before returning the stylized image\n","    return tf.squeeze(image, axis=0).numpy()\n","\n","# Load content and style images\n","content_path = '/content/download.jpeg'  # Provide path to your content image\n","style_path = '/content/download (1).jpeg'      # Provide path to your style image\n","\n","content_image = plt.imread(content_path)\n","style_image = plt.imread(style_path)\n","\n","# Preprocess images\n","content_image = tf.image.convert_image_dtype(content_image, tf.float32)\n","style_image = tf.image.convert_image_dtype(style_image, tf.float32)\n","\n","# Add batch dimension to the images\n","content_image = tf.expand_dims(content_image, axis=0)\n","style_image = tf.expand_dims(style_image, axis=0)\n","\n","# Remove extra dimensions\n","content_image = tf.squeeze(content_image, axis=0)\n","style_image = tf.squeeze(style_image, axis=0)\n","\n","# Perform style transfer\n","stylized_image = style_transfer(content_image, style_image)\n","\n","# Plot the results\n","plt.figure(figsize=(10, 10))\n","\n","plt.subplot(1, 3, 1)\n","plt.title('Content Image')\n","plt.imshow(content_image[0])  # Remove batch dimension when plotting\n","plt.axis('off')\n","\n","plt.subplot(1, 3, 2)\n","plt.title('Style Image')\n","plt.imshow(style_image[0])  # Remove batch dimension when plotting\n","plt.axis('off')\n","\n","plt.subplot(1, 3, 3)\n","plt.title('Stylized Image')\n","plt.imshow(stylized_image[0])  # Remove batch dimension when plotting\n","plt.axis('off')\n","\n","plt.show()\n"]}]}